"""
import boto3

s3 = boto3.client('s3', region_name='us-east-1',
				  region_name='us-east-1',
				  aws_access_key_id=AWS_KEY_ID,
				  aws_secret_access_key=AWS_SECRET)



response = s3.list_buckets()


IAM = Identity Access Managment Service



haremos uso de IAM, S3, SNS(simple notification service), comprehend, recognition
"""

"""
Your first boto3 client
Sam wants to cast off the shackles of only being able to use her computer for storage and compute. She is learning how to use the awesome power of the cloud to create data pipelines and automatically generate reports.

Before she can do all that, she needs to create her first boto3 client and check out what buckets already exist in S3.

Her AWS key and AWS secret key have been stored in AWS_KEY_ID and AWS_SECRET respectively.

In this exercise, you will help Sam by creating your first boto3 client to AWS!


"""
# Generate the boto3 client for interacting with S3
s3 = boto3.client('s3', region_name='us-east-1', 
                        # Set up AWS credentials 
                        aws_access_key_id=AWS_KEY_ID, 
                         aws_secret_access_key=AWS_SECRET)
# List the buckets
buckets = s3.list_buckets()

# Print the buckets
print(buckets)



"""
Multiple clients
Sam knows that she will often have to work with more than one service at once. She wants to practice creating two separate clients for two different services in boto3.

When she is building her workflows, she will make multiple Amazon Web Services interact with each other, with a script executed on her computer.

Her AWS key id and AWS secret have been stored in AWS_KEY_ID and AWS_SECRET respectively.

You will help Sam initialize a boto3 client for S3, and another client for SNS.

She will use the S3 client to list the buckets in S3. She will use the SNS client to list topics she can publish to (you will learn about SNS topics in Chapter 3).


"""

# Generate the boto3 client for interacting with S3 and SNS
s3 = boto3.client('s3', region_name='us-east-1', 
                         aws_access_key_id=AWS_KEY_ID, 
                         aws_secret_access_key=AWS_SECRET)

sns = boto3.client('sns', region_name='us-east-1', 
                         aws_access_key_id=AWS_KEY_ID, 
                         aws_secret_access_key=AWS_SECRET)

# List S3 buckets and SNS topics
buckets = s3.list_buckets()
topics = sns.list_topics()

# Print out the list of SNS topics
print(topics)



"""
Diving into buckets

S3 Components - Buckets(folder)

* Desktop folders

* Own permission policy

* Website storage

* Generate logs


S3 Components - Objects (files)


What can we do with buckets?

* Create Bucket

* List Buckets

* Delete Bucket


Creating a Bucket

import boto3

s3 = boto3.client('s3', region_name='us-east-1',
						aws_access_key_id=AWS_KEY_ID,
						aws_secret_access_key= AWS_SECRET)



Create bucket!

bucket = s3.create_bucket(Bucket='gid_requests')



bucket_response = s3.list_buckets()

buckets = bucket_response['Buckets']


response = s3.delete_bucket('gid-requests')




"""
"""
Creating a bucket
Sam is dipping her toes in the water, getting ready to build her first pipeline.

Get It Done is the app the City released for residents to report problems. There are lots of problems to report, and lots of data gets generated.




She will be picking up daily reports generated by Get It Done and placing them in the 'gim-staging' bucket. Then, she will clean the data and place the new dataset in the 'gim-processed' bucket.

She also wants to create a 'gim-test' bucket to experiment with.

Help Sam take the first step to her pipeline dreams. Help her create her first bucket, 'gim-staging'!
"""
import boto3

# Create boto3 client to S3
s3 = boto3.client('s3', region_name='us-east-1', 
                         aws_access_key_id=AWS_KEY_ID, 
                         aws_secret_access_key=AWS_SECRET)

# Create the buckets
response_staging = s3.create_bucket(Bucket='gim-staging')
response_processed = s3.create_bucket(Bucket='gim-processed')
response_test = s3.create_bucket(Bucket='gim-test')

# Print out the response
print(response_staging)


"""
Listing buckets
Sam has successfully created the buckets for her pipeline. Often, data engineers build in checks into the pipeline to make sure their previous operation succeeded. Sam wants to build in a check to make sure her buckets actually got created.

She also wants to practice listing buckets. Listing buckets will let her perform operations on multiple buckets using a for loop.

She has already created the boto3 client for S3, and assigned it to the s3 variable.

Help Sam get a list of all the buckets in her S3 account and print their names!

"""


"""


"""
# Get the list_buckets response
response = s3.list_buckets()
# print(response)

# Iterate over Buckets from .list_buckets() response
for bucket in response['Buckets']:
  
  	# Print the Name for each bucket
    print(bucket['Name'])


"""
Deleting a bucket
Sam is feeling more and more confident in her AWS and S3 skills. After playing around for a bit, she decides that the gim-test bucket no longer fits her pipeline and wants to delete it. It's starting to feel like dead weight, and Sam doesn't want it littering her beautiful bucket list.

She has already created the boto3 client for S3, and assigned it to the s3 variable.

Help Sam do some clean up, and delete the gim-test bucket.

"""


# Delete the gim-test bucket
s3.delete_bucket(Bucket='gim-test')

# Get the list_buckets response
response = s3.list_buckets()

# Print each Buckets Name
for bucket in response['Buckets']:
    print(bucket['Name'])



"""
Deleting multiple buckets
The Get It Done app used to be called Get It Made. Sam always thought it was a terrible name, but it got stuck in her head nonetheless.

When she was making the pipeline buckets, she used the gim- abbreviation for the old name. She decides to switch her abbreviation to gid- to accurately reflect the app's real (and better) name.

She has already set up the boto3 S3 client and assigned it to the s3 variable.

Help Sam delete all the buckets in her account that start with the gim- prefix. Then, help her make a 'gid-staging' and a 'gid-processed' bucket.


"""



"""
The Get It Done app used to be called Get It Made. Sam always thought it was a terrible name, but it got stuck in her head nonetheless.

When she was making the pipeline buckets, she used the gim- abbreviation for the old name. She decides to switch her abbreviation to gid- to accurately reflect the app's real (and better) name.

She has already set up the boto3 S3 client and assigned it to the s3 variable.

Help Sam delete all the buckets in her account that start with the gim- prefix. Then, help her make a 'gid-staging' and a 'gid-processed' bucket.

"""


# Get the list_buckets response
response = s3.list_buckets()

# Delete all the buckets with 'gim', create replacements.
for bucket in response['Buckets']:
  if 'gim' in bucket['Name']:
      s3.delete_bucket(Bucket=bucket['Name'])
    
s3.create_bucket(Bucket='gid-staging')
s3.create_bucket(Bucket='gid-processed')
  
# Print bucket listing after deletion
response = s3.list_buckets()
for bucket in response['Buckets']:
    print(bucket['Name'])

"""
Uploading and retrieving files


A Bucket

* A bucket has a name

* Name is a string

* Unique name in all of S3

* Contains many objects

An Object

* An object has a key

* Name is full path from bucket root

* Unique key in the bucket

* Can only be in one parent bucket


Creating the client

s3 = boto.client('s3', region_name='us-east-1',
				aws_access_key_id = AWS_KEY_ID,
				aws_secret_access_key=AWS_SECRET)




Uploading files

s3.upload_file(
	Filename = 'gid_requests_2019_01_01.csv',
	Bucket='gid-requests',
	Key = 'gid_requests_2019_01_01.csv'

)


Listing objects in a bucket

response = s3.list_objects(
 Bucket = 'gid-requests',
 MaxKeys=2,
 Prefix='gid_requests_2019_'

print(response)
)

'Contents':[{'key': 'gid_requests_2018_12_30.csv',
	'LastModified':datetime.datetime(2019, 4, 18, 21, 38, 30)}]

Getting object metadata

response = s3.head_object(
	Bucket='gid-requests',
	Key='gid_requests_2018_12_30.csv'
)

print(response)


Downloading files

s3.download_file(
	Filename = 'gid_request_downed.csv',
	Bucket='gid-requests',
	Key='gid_requests_2018_12_30.csv'

Deleting objects

s3.delete_object(
	Bucket='gid-requests',
	Key='gid_requests_2018_12_30.csv'
)



Summary

* Buckets are like folders

* Objects are like files

*boto3.client()
*s3.upload_file()
*s3.list_objects()
*s3.head_object()

*s3.download_file()

*s3.delete_object()



Putting files in the cloud
Now that Sam knows how to create buckets, she is ready to automate a tedious part of her job. Right now, she has to download the latest files from the City of San Diego Open Data Portal, aggregate them, and share them with management.

Sharing an analysis with others is a common, yet tedious data science task. Automating these steps will allow Sam to focus on cooler projects, while keeping her management happy.

In the last lesson, Sam has already created the gid-staging bucket. She has already downloaded the files from the URLs, analyzed them, and wrote the results to final_report.csv.

She has also already initialized the boto3 S3 client and assigned it to the s3 variable.

Help Sam upload final_report.csv to the gid-staging bucket!



"""

# Upload final_report.csv with key 2019/final_report_01_01.csv
s3.upload_file(Bucket='gid-staging', 
               # Set filename and key
               Filename='final_report.csv', 
               Key='2019/final_report_01_01.csv')

# Get object metadata and print it
response = s3.head_object(Bucket='gid-staging', 
                       Key='2019/final_report_01_01.csv')

# Print the size of the uploaded object
print(response['ContentLength'])


"""
Spring cleaning
Sam's pipeline has been running for a long time now. Since the beginning of 2018, her automated system has been diligently uploading her report to the gid-staging bucket.

In City governments, record retention is a huge issue, and many government officials prefer not to keep records in existence past the mandated retention dates.

As time has passed, the City Council asked Sam to clean out old CSV files from previous years that have passed the retention period. 2018 is safe to delete.

Sam has initialized the client and assigned it to the s3 variable. Help her clean out all records for 2018 from S3!
"""



# List only objects that start with '2018/final_'
"""
 response = s3.list_objects(
 Bucket = 'gid-requests',
 MaxKeys=2,
 Prefix='gid_requests_2019_'


"""

response = s3.list_objects(Bucket='gid-staging', 
                           Prefix='2018/final_')

print(response)

# Iterate over the objects
if 'Contents' in response:
  for obj in response['Contents']:
      # Delete the object
      s3.delete_object(Bucket='gid-staging', Key=obj['Key'])

# Print the keys of remaining objects in the bucket
response = s3.list_objects(Bucket='gid-staging')

for obj in response['Contents']:
  	print(obj['Key'])



"""
Keeping objects secure


Why care about permissions?

df = pd.read_csv('https://gid-staging.s3.amazonaws.com/potholes.csv')

HTTPError: HTTP Error 403: Forbidden


Permission Allowed!

# Generate the boto3  client for interacting with s3

s3.boto3.client('s3', region_name='us-east-1',
					aws_access_key_id=AWS_KEY_ID,
					aws_secret_access_key=AWS_SECRET)

# Use Client to download a file

s3.download_file(
	Filename='potholes.csv',
	Bucket='gid-requests',
	Key='potholes.csv')


AWS Permissions Systems

IAM, Bucket Policy, ACL, Presigned URL

ACL : access control lists
let us set permissions on specific objects within a bucket. Lastly, presigned URLs let us provide temporary access to an object.


ACLs are entities attached to objects in S3. We will focus on 2 types of ACL: private and public-read.

ACLs
Upload File

s3.upload_file(
Filename = 'potholes.csv', Bucket='gid-requests', Key='potholes.csv'
)

Set ACL to 'public-read'

s3.put_object_acl(
	Bucket='gid-requests', Key='potholes.csv', ACL='public-read')

Setting Acls on upload

Upload file with 'public-read' ACL

s3.upload_file(
 Bucket='gid-requests',
 Filename='potholes.csv',
 Key='potholes.csv',
 ExtraArgs={'ACL':'public-read'}

 Accessing public objects

 S3 Object URL Template

 https://{bucket}.s3.amazonaws.com/{key}

 URL for Key='2019/potholes.csv'
https://gid-requests.s3.amazonaws.com/2019/potholes.csv


Generating public object URL

Generate Object URL String

url = "https://{}.s3.amazonaws.com/{}".format(
"gid-requests",
"2019/potholes.csv"

'https://gid-requests.s3.amazonaws.com/2019/potholes.csv'

# Read the URL into Pandas

df = pd.read_csv(url)
)
)
"""

"""

Got It!
1. Keeping objects secure
In the previous chapter, we learned how to create and delete buckets and upload objects inside of them. Often we work with private data or data we want only certain users to see. This is where AWS permission system comes in.

2. Why care about permissions?
AWS defaults to denying permission. That means if we upload a file, by default only our key and secret have access to it. This behavior may sound annoying, but it's much more secure to opt-in to allowing public access to our data than to have it be public by default. If we or our colleague tries to download a file with pandas, S3 won't let them. Even if it's in the same script that uploaded it!

3. Why care about permissions?
But if we initialize the s3 client with our credentials, it will work! Let's learn how to protect (and expose) our buckets and objects.

4. AWS Permissions Systems
There are 4 ways we can control permissions in S3. We can use IAM to control users' access to AWS services, buckets, and objects. We used it in lesson 1 to give permissions by attaching IAM policies to a user. IAM applies across all AWS services. Bucket policies give us control on the bucket and the objects within it. ACLs or access control lists let us set permissions on specific objects within a bucket. Lastly, presigned URLs let us provide temporary access to an object.

5. AWS Permissions Systems
IAM and Bucket Policies are great in multi-user environments. But since Sam isn't managing one, we will focus on ACLs and presigned URLs.

6. ACLs
ACLs are entities attached to objects in S3. We will focus on 2 types of ACL: private and public-read.

7. ACLs
Say we upload a file. By default, its ACL is 'private'. Let's set the ACL to 'public-read' using s3_put_object_acl method. Now anyone in the world can download this file.

8. Setting ACLs on upload
We can also set the ACL as 'public-read' on upload. We pass a dictionary with key ACL and value 'public-read' to the ExtraArgs parameter.

9. Accessing public objects
Once we have a public object, anyone in the world can access it at the URL with the format of bucket.s3.amazonaws.com/objectKey. A URL for an object with the key 2019/potholes.csv and bucket gid-requests will look like this.

10. Generating public object URL
We can use Python's nifty string format method to generate a public URL for an object in S3. This is different than a pre-signed URL which we will cover in the next lesson. We create a string where the bucket name and the object key are empty brackets. We call the format method, passing positional arguments of bucket - gid-requests, and key - 2019/potholes.csv. We can now pass this URL to something like Pandas with no problems.

11. How access is decided
So how does this work together? When a request comes in if it's a presigned URL, it will allow the download.

12. How access is decided
If it's not pre-signed, it will check the policies to make sure they allow the download. AWS's default behavior is to deny access.

"""
"""
13. Review
In this lesson, we learned about AWS permissions systems. IAM answers "What can this user do in AWS?" Bucket Policies answer "Who can access this S3 bucket?" ACLs answer "Who can access this object"? And presigned URLs let us grant temporary access to objects.

14. Review
We can set ACLs to public-read or private on existing objects.

15. Review
Or we can upload with a specified ACL using ExtraArgs

16. Review
Lastly, we can generate public URLs for objects with a public-read ACL.

17. Let's practice!
Now that we have a grasp of basic AWS permissions, let's practice!
"""
"""
Uploading a public report
As you saw in Chapter 1, Get It Done is an App that lets residents report problems like potholes and broken sidewalks.

The data from the app is a very hot topic political issue. Residents keep saying that the City does not distribute work evenly across neighborhoods when issues are reported. The City Council wants to be transparent with the public and has asked Sam to publish the aggregated Get It Done reports and make them publicly available.

Sam has initialized the boto3 S3 client and assigned it to the s3 variable.

In this exercise, you will help her increase government transparency by uploading public reports to the gid-staging bucket.
"""

# Upload the final_report.csv to gid-staging bucket
s3.upload_file(
  # Complete the filename
  Filename='./final_report.csv', 
  # Set the key and bucket
  Key='2019/final_report_2019_02_20.csv', 
  Bucket='gid-staging',
  # During upload, set ACL to public-read
  ExtraArgs = {
    'ACL': 'public-read'})



"""
Making multiple files public
Transparency is important to City Council. They want to empower residents to analyze Get It Done requests and how they get prioritized.

They asked Sam to make all previous Get It Done aggregated reports since the beginning of 2019 public as well.
Sam has initialized the boto3 S3 client and assigned it to the s3 variable.

In this exercise, you will help Sam open up the data by setting the ACL of every object in the gid-staging bucket to public-read, opening up the objects to the world!


"""


# List only objects that start with '2019/final_'
response = s3.list_objects(
    Bucket='gid-staging', Prefix='2019/final_')

# Iterate over the objects
for obj in response['Contents']:

    # Give each object ACL of public-read
    s3.put_object_acl(Bucket='gid-staging', 
                      Key=obj['Key'], 
                      ACL='public-read')
    
    # Print the Public Object URL for each object
    print("https://{}.s3.amazonaws.com/{}".format( 'gid-staging', obj['Key']))


"""
Downloading private files

Download File

s3.download_file(
Filename='pothholes_local.csv',
Bucket = 'gid-staging',
Key='2019/potholes_private.csv')

Read From Disk

pd.read_csv('./potholes_local.csv')

obj = s3.get_object(Bucket='gid-requests', key='2019/potholes.csv')

print(obj)


pd.read_csv('./potholes_local.csv')



Accessing private Files

Get the object

obj = s3.get_object(
	Bucket='gid-requests',
	Key='2019/potholes.csv'
)

Read StreamingBody into Pandas

pd.read_csv(obj['Body'])

Pre-signed URLs
Upload a file

s3.upload_file(
	Filename='./potholes.csv',
	Key='potholes.csv',
	Bucket='gid-requests'

)


Generate Presigned URL

share_url = s3.generate_presigned_url(
	ClientMethod='get_object',
	ExpiresIn=3600,
	Params={'Bucket': 'gid-requests, 'Key':'potholes.csv'}
)

Open in Pandas

pd.read_csv(share_url)


Load multiple files into one DataDrame

# Create list to hold our DataFrames

df_list = []

# Request the list of cvs's from s3 with prefix; Get contents

response = s3.list_objects(
	Bucket='gid-requests',
	Prefix='2019/'
)

# Get response contents

request_files = response['Contents']
# Iterate over each object

for file in request_files:
	obj = s3.get_object=(Bucket='gid-requests', Key=file['Key'])

	# Read it as DataFrame
	obj_df = pd.read_csv(obj['Body'])


	# Append DataFrame to list

	df_list.append(obj_df)


# Concatenate all the DataFrames in the list

df = pd.concat(df_list)

# Preview the DataFrame

df.head()
Download then open

s3.download_file()


Open directly

s3.get_object()


Generate presigned UL


s3.generate_presigned_url()

"""


"""
Generating a presigned URL
Sam got a special request from City Council to analyze whether the City is prioritizing requests in District 11, while de-prioritizing requests in the less affluent district 12. They asked her to keep this report confidential, as they would like to see it before it goes public to the media.

Sam has generated the report and is ready to share it with the City council, but making it public makes her too paranoid. She decided to provide the Council with a presigned URL so they can temporarily access the report for 1 hour.

She has already initialized the boto3 S3 client and assigned it to the s3 variable.

Help her generate a presigned URL valid for 1 hour to 'final_report.csv' in the 'gid-staging' bucket. Then, print it out for the City Council!

"""


"""
Sam got a special request from City Council to analyze whether the City is prioritizing requests in District 11, while de-prioritizing requests in the less affluent district 12. They asked her to keep this report confidential, as they would like to see it before it goes public to the media.

Sam has generated the report and is ready to share it with the City council, but making it public makes her too paranoid. She decided to provide the Council with a presigned URL so they can temporarily access the report for 1 hour.

She has already initialized the boto3 S3 client and assigned it to the s3 variable.

Help her generate a presigned URL valid for 1 hour to 'final_report.csv' in the 'gid-staging' bucket. Then, print it out for the City Council!

"""

# Generate presigned_url for the uploaded object
share_url = s3.generate_presigned_url(
  # Specify allowable operations
  ClientMethod='get_object',
  # Set the expiration time
  ExpiresIn=3600,
  # Set bucket and shareable object's name
  Params={'Bucket': 'gid-staging','Key': 'final_report.csv'}
)

# Print out the presigned URL
print(share_url)


"""
Opening a private file
The City Council wants to see the bigger trend and have asked Sam to total up all the requests since the beginning of 2019. In order to do this, Sam has to read daily CSVs from the 'gid-requests' bucket and concatenate them. However, the gid-requests files are private. She has access to them via her key, but the world cannot access them.

In this exercise, you will help Sam see the bigger picture by reading these private files into pandas and concatenating them into one DataFrame!

She has already initialized the boto3 S3 client and assigned it to the s3 variable. She has listed all the objects in gid-requests in the response variable

"""

df_list =  [ ] 

for file in response['Contents']:
    # For each file in response load the object from S3
    obj = s3.get_object(Bucket='gid-requests', Key=file['Key'])
    # Load the object's StreamingBody with pandas
    print(obj)
    obj_df = pd.read_csv(obj['Body'])
    # Append the resulting DataFrame to list
    df_list.append(obj_df)

# Concat all the DataFrames with pandas
df = pd.concat(df_list)

# Preview the resulting DataFrame
df.head()




"""

Sharing files through a website


HTML table in Pandas

Convert DataFrame to html

df.to_html('table_agg.html', render_links=True)

df.to_html('table_agg.html',
		   render_links=True,
		   columns['service_name', 'request_count', 'info_link'],
		   border=0)

Uploading an HTML file to S3

Upload an HTML file to S3

s3.upload_file(
 Filename='./table_agg.html',
 Bucket='datacamp-website',
 Key='table.html',

 ExtraArgs = {
	'ContentType':'text/html',
	'ACL': 'public-read'
 }
)

Accesing HTML file

S3 object URL Template

https://{bucket}.s3.amazonaws.com/{key}

https://datacamp-website.s3.amazonaws.com/table.html
Uploading other types of content

Upload an image file to S3

s3.upload_file(
 Filename='./plot_image.png',
 Bucket='datacamp-website',
 Key='plot_image.png',
 ExtraArgs={
  "ContentType": 'image/png',
  'ACL': 'public-read'		
 })
IANA  Medi Types

*JSON: application/json
*PNG: image/png
*PDF: application/pdf
*CSV: text/csv

Generating an index page


# List the gid-reports bucket objects starting with 2019/

r = s3.list_objects(Bucket='gid-reports', Prefix='2019/')

# Convert the response contents to DataFrame

objects_df = pd.DataFrame(r['Contents'])

# Create a column  "Link" that contains website url + key

base_url = "http://datacamp-website.s3.amazonaws.com/"

objects_df['Link'] = base_url + objects_df['Key']


# Write DataFrame to html

objects_df.to_html('report_listing.html',
				   columns=['Link', 'lastModified', 'Size'],
				   render_links=True)

Uploading index page

Upload an HTML file to S3

s3.upload_file(
	Filename='./report_listing.html',
	Bucket='datacamp-website',
	Key='datacamp-website',
	ExtraArgs = {
		
		'ContentType': 'text/html',
		'ACL':'public-read'

	}
)


Review
* HTML Table in Pandas(df.to_html('table.html'))

* Upload HTML file(ContentType: text/html)

* Upload Image file(ContentType: image/png)

* Share the URL for our html page



"""
"""
Generate HTML table from Pandas
Residents often complain that they don't know the full range of services offered through the Get It Done application.

In an effort to streamline the communication between the City government and the public, the City Council asked Sam to generate a table of all the services in the Get It Done system.

The system is dynamic and grows on a weekly basis, adding additional services. Sam didn't want to waste time updating the file manually, and felt like she can automate the process.

She loaded the DataFrame of available services into the services_df

"""

# Generate an HTML table with no border and selected columns
services_df.to_html('./services_no_border.html',
           # Keep specific columns only
           columns=['service_name', 'link'],
           # Set border
           border=False)

# Generate an html table with border and all columns.
services_df.to_html('./services_border_all_columns.html', 
           render_links=True,
           border=True)



"""
Upload an HTML file to S3
When the Streets Operations manager heard of what Sam has been working on, he asked her to build him a dashboard of Get It Done requests.

He wants to use the dashboard to staff and schedule his team accordingly.

Sam generated a nice dashboard html file with the Python bokeh charting library:



She wants to serve it as a website, providing an interactive dashboard to members of Streets Operations.

Letting S3 serve the dashboard as a site lets her write a script that continuously updates the generated HTML file and keeps the Streets Operations team updated on the latest requests.

She has already initialized the boto3 S3 client and assigned it to the s3 variable.
"""

# Upload the lines.html file to S3
s3.upload_file(Filename='lines.html', 
               # Set the bucket name
               Bucket='datacamp-public', Key='index.html',
               # Configure uploaded file
               ExtraArgs = {
                 # Set proper content type
                 'ContentType':'text/html',
                 # Set proper ACL
                 'ACL': 'public-read'})

# Print the S3 Public Object URL for the new file.
print("http://{}.s3.amazonaws.com/{}".format('datacamp-public', 'index.html'))




"""
Case Study: Generating a Report Repository

The steps
Prepare the data
* Download files for the month from the raw data bucket
* Concatenate them into one csv
* Create an aggregated DataFrame
* Write the DataFrame to CSV and HTML
* Generate a Bokeh plot, save as HTML
*Create gid-reports bucket
* Upload all the three files for the moenth to s3

* Generate an index.html file that list all the files 

*  Get the website URl
Raw data bucket
gid-requests

* Private files
* Daily CSVs of requests from the App

* Raw data


Read raw data files
# Create list to hold our DataFrames

df_list = []
# Request the list of csv's from s3 with prefix;  Get contents

response = s3.list_objects(
	Bucket='gid-requests',
	Prefix='2019_jan'# esto me permite filtrar
)

# Get response contents

request_files = respose['Contents']

# Iterate over each object

for file in request_files:
	obj = s3.get_object(Bucket='gid-requests', Key=file['key'])

	# Read it as  DataFrame

	obj_df = pd.read_csv(obj['Body'])


	# Append DataFrame to list

df_list.append(obj_df)

# Concatenate all the DataFrames in the list

df = pd.concat(df_list)

# Preview the DataFrame

df.head()
# Perform some aggregation

* df.to_csv('jan_final_reports.csv')

* df.to_html('jan_final_report.html')

jan_final_chart.html

# Upload Aggregated CSV to S3

s3.upload_file(Filename='./jan_final_report.csv',
					Key='2019/jan/final_report.csv',
					Bucket='gid-reports',
					ExtraArgs = {'ACL': 'public-read'}
# Upload HTML Table

s3.upload_file(Filename='./jan_final_report.html',
				Key = '2019/jan/final_report.html',
				Bucket = 'gid-reports',
				ExtraArgs= {
					'ContentType':'text/html',
					'ACL': 'public-read'
				} 
)


# Upload Aggregated chart to S3

s3.upload_file(Filename = './jan_final_chart.html',
				Key = '2019/jan/final_chart.html',
				Bucket='gid-reports',
				ExtraArgs = {
						'ContentType': 'text/html',
						'ACL': 'public-read'
				}
)


Create index.html

# List the gid-reports bucket objects starting with 2019/
r = s3.list_objects(Bucket='gid-reports',Prefix= '2019/')


# Convert the response contents to DataFrame

objects_df = pd.DataFrame(r['Contents'])


# Create a column "link" that contains website url + key


base_url = "https://gid-reports.s3.amazonaws.com"

objects_df['Link'] = base_url + objects_df['Key']

# Write DataFrame to html

objects_df.to_html('report_listing.html',
					columns=['Link', 'LastModified', 'Size'],
					render_links=True 
)

# Upload the file to gid-reports bucket root.

s3.upload_file(
	Filename='./report_listing.html',
	Key = 'index.html',
	Bucket='gid-reports',
	ExtraArgs = {
		'ContentType': 'text/html',
		'ACL': 'public-read'
	}

)

Bucket wetsite URL * 

"http://gid-reports.s3.amazonaws.com/index.html"

)

"""

"""
Combine daily requests for February
It's been a month since Sam last ran the report script, and it's time for her to make a new report for February.

She wants to upload new reports for February and update the file listing, expanding on the work she completed during the last video lesson:

She has already created the boto3 S3 client and stored in the s3 variable. She stored the contents of her objects in request_files.

You will help Sam aggregate the requests from February by downloading files from the gid-requests bucket and concatenating them into one DataFrame!
"""
df_list = [] 

# Load each object from s3
for file in request_files:
    s3_day_reqs = s3.get_object(Bucket='gid-requests', 
                                Key=file['Key'])
    # Read the DataFrame into pandas, append it to the list
    day_reqs = pd.read_csv(s3_day_reqs['Body'])
    df_list.append(day_reqs)

# Concatenate all the DataFrames in the list
all_reqs = pd.concat(df_list)

# Preview the DataFrame
all_reqs.head()


"""
# Upload the generated CSV to the gid-reports bucket

She has already created the boto3 S3 client in the s3 variable.

Help her publish this month's request statistics.

Write agg_df to CSV and HTML files, and upload them to S3 as public files.
"""

# Write agg_df to a CSV and HTML file with no border
agg_df.to_csv('./feb_final_report.csv')
agg_df.to_html('./feb_final_report.html', border=0)

# Upload the generated CSV to the gid-reports bucket
s3.upload_file(Filename='./feb_final_report.csv', 
	Key='2019/feb/final_report.html', Bucket='gid-reports',
    ExtraArgs = {'ACL': 'public-read'})

# Upload the generated HTML to the gid-reports bucket
s3.upload_file(Filename='./feb_final_report.html', 
	Key='2019/feb/final_report.html', Bucket='gid-reports',
    ExtraArgs = {'ContentType': 'text/html', 
                 'ACL': 'public-read'})


"""
Update index to include February
In the previous two exercises, Sam has:

Read the daily Get It Done request logs for February.
Combined them into a single DataFrame.
Generated a DataFrame with aggregated metrics (request counts by type)
Wrote that DataFrame to a CSV and HTML final report files.
Uploaded these files to S3.
Now, she wants these files to be accessible through the directory listing. Currently, it only shows links for January reports:

She has created the boto3 S3 client and stored it in the s3 variable.

Help Sam generate a new directory listing with the February's uploaded reports and store it in a DataFrame.

"""

# List the gid-reports bucket objects starting with 2019/
objects_list = s3.list_objects(Bucket='gid-reports', Prefix='2019/')

# Convert the response contents to DataFrame
objects_df = pd.DataFrame(objects_list['Contents'])

# Create a column "Link" that contains Public Object URL
base_url = "http://gid-reports.s3.amazonaws.com/"
objects_df['Link'] = base_url + objects_df['Key']

# Preview the resulting DataFrame
objects_df.head()


"""
Sam is almost done! In the last exercise, she generated a new directory listing, storing it in the objects_df variable:

Sam has created the boto3 S3 client in the s3 variable. objects_df is populated with the new directory listing from the previous exercise.

The next step is to write objects_df to an HTML file, and upload it to S3 replacing the current 'index.html' file.

Help Sam update the directory listing, letting the public access reports for February as well as January!
"""

# Write objects_df to an HTML file
objects_df.to_html('report_listing.html',
    # Set clickable links
    render_links=True,
	# Isolate the columns
    columns=['Link', 'LastModified', 'Size'])

# Overwrite index.html key by uploading the new file
s3.upload_file(
  Filename='./report_listing.html', Key='index.html', 
  Bucket='gid-reports',
  ExtraArgs  = {
    'ContentType': 'text/html', 
    'ACL': 'public-read'
  })







#---------------------------------------------------------------------------------------------------------


"""

SNS Topics(simple notification service)

we can sent email, text messages and notification

Publisher ------SNS Topic-----SNS subscriber

sns = boto3.client('sns',
     				region_name='us-east-1',
     				aws_access_key_id=AWS_KEY_ID,
     				aws_secret_access_key=AWS_SECRET)




response = sns.create_topic(Name='city_alerts')

topic_arn = response['TopicArn']

or... a shortcut

sns.create_topic(Name='city_alerts')['TopicArn']

Listing topics

response = sns.list_topics()

sns.delete_topic(TopicArn='arn:aws:sns:us-east-1:320333787981:city_alerts')



"""

"""
Creating a Topic
Sam has been doing such a great job with her new skills, she got a promotion.

With her new title of Information Systems Analyst 3 (as opposed to 2), she has gotten a tiny pay bump in exchange for a lot more work.

Knowing what she can do, the City Council asked Sam to prototype an alerting system that will alert them when any department has more than 100 Get It Done requests outstanding.

They would like for council members and department directors to receive the alert.

Help Sam use her new knowledge of Amazon SNS to create an alerting system for Council!
"""

# Initialize boto3 client for SNS
sns = boto3.client('sns', 
                   region_name='us-east-1', 
                   aws_access_key_id=AWS_KEY_ID, 
                   aws_secret_access_key=AWS_SECRET)

# Create the city_alerts topic
response = sns.create_topic(Name='city_alerts')
c_alerts_arn = response['TopicArn']

# Re-create the city_alerts topic using a oneliner
c_alerts_arn_1 = sns.create_topic(Name='city_alerts')['TopicArn']

# Compare the two to make sure they match
print(c_alerts_arn == c_alerts_arn_1)



"""
Creating multiple topics
Sam suddenly became a black sheep because she is responsible for an onslaught of text messages and notifications to department directors.

No one will go to lunch with her anymore!

To fix this, she decided to create a general topic per department for routine notifications, and a critical topic for urgent notifications.

Managers will subscribe only to critical notifications, while supervisors can monitor general notifications.

For example, the streets department would have 'streets_general' and 'streets_critical' as topics.

She has initialized the SNS client and stored it in the sns variable.

Help Sam create a tiered topic structure... and have friends again!

"""
"""
Creating multiple topics
Sam suddenly became a black sheep because she is responsible for an onslaught of text messages and notifications to department directors.

No one will go to lunch with her anymore!

To fix this, she decided to create a general topic per department for routine notifications, and a critical topic for urgent notifications.

Managers will subscribe only to critical notifications, while supervisors can monitor general notifications.

For example, the streets department would have 'streets_general' and 'streets_critical' as topics.

She has initialized the SNS client and stored it in the sns variable.

Help Sam create a tiered topic structure... and have friends again!

"""

# Create list of departments
departments = ['trash', 'streets', 'water']

for dept in departments:
    # For every department, create a general topic
    sns.create_topic(Name="{}_general".format(dept))
    
    # For every department, create a critical topic
    sns.create_topic(Name="{}_critical".format(dept))

# Print all the topics in SNS
response = sns.list_topics()
print(response['Topics'])




"""
Deleting multiple topics
It's hard to get things done in City government without good relationships. Sam is burning bridges with the general topics she created in the last exercise.

People are shunning her because she is blowing up their phones with notifications.

She decides to get rid of the general topics per department completely, and keep only critical topics.

Sam has created the boto3 client for SNS and stored it in the sns variable.

Help Sam regain her status in the bureaucratic social hierarchy by removing any topics that do not have the word critical in them.


"""
# Get the current list of topics
topics = sns.list_topics()['Topics']
# print(topics)

for topic in topics:
  # For each topic, if it is not marked critical, delete it
  if "critical" not in topic['TopicArn']:
    sns.delete_topic(TopicArn=topic['TopicArn'])
    
# Print the list of remaining critical topics
print(sns.list_topics()['Topics'])


"""
SNS subscriptions
Creating an SMS subscription.

sns = boto3.client('sns', 
                    region_name='us-east-1',
                    aws_access_key_id=AWS_KEY_ID,
                    aws_secret_access_key=AWS_SECRET
)

response = sns.subscribe(
  TopicArn = 'arn:aws:sns:us-east-1:320333787981:city_alerts',
  Protocol = 'SMS',

  Endpoint = '+13125551123'

)

Creating an email subscription

response = sns.subscribe(
  TopicArn = 'arn:aws:sns:us-east-1:320333787981:city_alerts',
  Protocol='email',
  Endpoint='max@maksimize.com'

)

Listing subcriptions by Topic

sns.list_subscriptions_by_topic(
  TopicArn='arn:aws:sns:us-east-1:320333787981:city_alerts'
)

sns.list_subscriptions(['Subscriptions'])

Deleting subscriptions

sns.unsubscribe(
  SubscriptionArn='arn:aws:sns:us-east-1:320333787981:city_alerts:9f2dad1d-8844-4fe8'
)


response = sns.list_subscriptions_by_topic(
  TopicArn:'arn:aws:sns:us-east-1:320333787981:city_alerts'
)

subs = response['Subscriptions']


Unsubscribe SMS subscriptions

for sub in subs:
  if sub['Protocol'] == 'sms':
    sns.unsubscribe(sub['SubscriptionArn'])


"""

"""
Subscribing to topics
Many department directors are already receiving critical notifications.

Now Sam is ready to start subscribing City Council members.

She knows that they can be finicky, and elected officials are not known for their attention to detail or tolerance for failure.

She is nervous, but decides to start by subscribing the friendliest Council Member she knows. She got Elena Block's email and phone number.

Sam has initialized the boto3 SNS client and stored it in the sns variable.

She has also stored the topic ARN for streets_critical in the str_critical_arn variable.

Help Sam subscribe her first Council member to the streets_critical topic!
"""
# Subscribe Elena's phone number to streets_critical topic
resp_sms = sns.subscribe(
  TopicArn = str_critical_arn, 
  Protocol='sms', Endpoint="+16196777733")

# print(resp_sms)

# Print the SubscriptionArn
print(resp_sms['SubscriptionArn'])

# Subscribe Elena's email to streets_critical topic.
resp_email = sns.subscribe(
  TopicArn = str_critical_arn, 
  Protocol='email', Endpoint="eblock@sandiegocity.gov")

# Print the SubscriptionArn
print(resp_email['SubscriptionArn'])


"""
Creating multiple subscriptions
After the successful pilot with Councilwoman Elena Block, other City Council members have been asking to be signed up for alerts too.

Sam decides that she should manage subscribers in a CSV file, otherwise she would lose track of who needs to be subscribed to what.

She creates a CSV named contacts and decides to subscribe everyone in the CSV to the streets_critical topic.

She has created the boto3 SNS client in the sns variable, and the streets_critical topic ARN is in the str_critical_arn variable.

Sam is going from being a social pariah to being courted by multiple council offices.

Help her solidify her position as master of all information by adding all the users in her CSV to the streets_critical topic!


"""

# For each email in contacts, create subscription to street_critical
for email in contacts['Email']:
  sns.subscribe(TopicArn = str_critical_arn,
                # Set channel and recipient
                Protocol = 'email',
                Endpoint = email )

# List subscriptions for streets_critical topic, convert to DataFrame
response = sns.list_subscriptions_by_topic(
  TopicArn  = str_critical_arn)
subs = pd.DataFrame(response['Subscriptions'])

# Preview the DataFrame
subs.head()


"""
Deleting multiple subscriptions
Now that Sam has a maturing notification system, she is learning that the types of alerts she sends do not bode well for text messaging.

SMS alerts are great if the user can react that minute, but "We are 500 potholes behind" is not something that a Council Member can jump up and fix.

She decides to remove all SMS subscribers from the streets_critical topic, but keep all email subscriptions.

She created the boto3 SNS client in the sns variable, and the streets_critical topic ARN is in the str_critical_arn variable.

In this exercise, you will help Sam remove all SMS subscribers and make this an email only alerting system.

"""
# List subscriptions for streets_critical topic.
response = sns.list_subscriptions_by_topic(
 TopicArn  = str_critical_arn)

# For each subscription, if the protocol is SMS, unsubscribe
for sub in response['Subscriptions']:
  if sub['Protocol'] == 'sms':
    sns.unsubscribe(SubscriptionArn=sub['SubscriptionArn'])

# List subscriptions for streets_critical topic in one line
subs = sns.list_subscriptions_by_topic(
  TopicArn=str_critical_arn)['Subscriptions']

# Print the subscriptions
print(subs)



"""
Sending messages

Publishing to a Topic

response = sns.publish(
  TopicArn = 'arn:aws:sns:us-east-1:320333787981:city_alerts',
  Message = 'Body text of SMS or e-mail',
  Subject = 'Subject Line for Email'
)

Sending customs messages

response = client.publish(
  TopicArn: 'arn:aws:sns:us-east-1:320333787981:city_alerts',
  Message = 'There are {} reports outstanding'.format(num_of_reports),
  Subject = 'Subject Line for Email'
)

Sending a single SMS

response = sns.publish(
  PhoneNumber = '+13121233211',
  Message = 'Body text of SMS or e-mail'
)

Not a good long term practice

* One-off texts = getting stuff done

* Topics and subscribers = maintenability
Publish to a topic

* Have to have a topic
* Our topic has to have subscriptions

* Better for multiple receivers

* Easier list management

Send a single SMS
* Don't need a topic
* Don't need subscriptions
* Just sends a message to a phone number

* Email option not available

"""
"""
Sending an alert
Elena Block, Sam's old friend and council member is running for office and potholes are a big deal in her district. She wants to help fix it.

Elena asked Sam to adjust the streets_critical topic to send an alert if there are over 100 unfixed potholes in the backlog.

Sam has created the boto3 SNS client in the sns variable. She stored the streets_critical topic ARN in the str_critical_arn variable.

Help Sam take the next step.

She needs to check the current backlog count and send a message only if it exceeds 100.

The fate of District 12, and the results of Elena's election rest on your and Sam's shoulders.

"""
"""
Sending a single SMS message
Elena asks Sam outside of work (per regulation) to send some thank you SMS messages to her largest donors.

Sam believes in Elena and her goals, so she decides to help.

She decides writes a quick script that will run through Elena's contact list and send a thank you text.

Since this is a one-off run and Sam is not expecting to alert these people regularly, there's no need to create a topic and subscribe them.

Sam has created the boto3 SNS client and stored it in the sns variable. The contacts variable contains Elena's contacts as a DataFrame.

Help Sam put together a quick hello to Elena's largest supporters!
"""

# Loop through every row in contacts
for idx, row in contacts.iterrows():
    
    # Publish an ad-hoc sms to the user's phone number
    response = sns.publish(
        # Set the phone number
        PhoneNumber = str(row['Phone']),
        # The message should include the user's name
        Message = 'Hello {}'.format(row['Name'])
    )
   
    print(response)



"""
Case Study: Building a notification system
Topic Set Up
* Create the topic
* Download the contact list csv
* Create topics for each service


* Subscribe the contacts to their respective topics

Get the aggregated numbers

* Download the monthly get it done report
* Get the count of Potholes
* Get the count of illegal dumping notifications

Send Alerts
* if potholes exceeds 100, send alert

* if illegal dumping exceeds 30, send alert

Topic set up
Initialize SNS client

sns = boto3.client('sns',
                   region_name='us-east-1',
                   aws_access_key_id=AWS_KEY_ID,
                   aws_secret_access_key=AWS_SECRET)

Create topics and store their ARNs

trash_arn = sns.create_topic(Name="trash_notifications")['TopicArn']

streets_arn = sns.create_topic(Name="streets_notifications")['TopicArn']

Subscribing users to topics

contacts = pd.read_csv('http://gid-staging.s3.amazonaws.com/contacts.csv')

Create  subscribe_user method

def subscribe_user(user_row):
  if user_row['Department'] == 'trash':
     sns.subscribe(TopicArn = trash_arn, Protocol='sms', Endpoint=str(user_row['Phone']))
     sns.subscribe(TopicArn = trash_arn, Protocl = 'email', Endpoint= user_row['Email'])
  
  else:
    sns.subscribe(TopicArn = streets_arn, Protocol='sms', Endpoint=str(user_row['Phone']))

    sns.subscribe(TopicArn = streets_arn, Protocol='email', Endpoint=user_row['Email'])


Apply the subscribe_user method to every row

contas.apply(subscribe_user, axis=1)

Load January's report into a  DataFrame

df = pd.read_csv('http://gid-reports.s3.amazonaws.com/2019/feb/final_report.csv')

Set the index so we can access counts by service name directly

df.set_index('service_name', inplace=True)

get the aggregated numbers

trash_violations_count = df.at['Illegal Dumping', 'count']
streets_violations_count = df.at['Pothole', 'count']
send alerts
if trash_violations_count > 100:
  # Construct the message to send
  message = "Trash violations count is now {}".format(trash_violations_count)
  # Send message
  sns.publish(TopicArn = trash_arn,
              Message = message,
              Subject = "Trash Alert"
  )

if streets_violations_count > 30:

   # Construct the message to send

   message = "Streets violations count is now {}".format(streets_violations_count)

   # Send message

   sns.publish(TopicArn = streets_arn,
                Message = message,
                Subject = "Streets Alert"
   )


"""
"""
Creating multi-level topics
The City Council asked Sam to create a critical and extreme topic for each department.

The critical topic will trigger alerts to staff and managers.

The extreme topics will trigger alerts to politicians and directors - it means the thresholds are completely exceeded.

For example, the trash department will have a trash_critical and trash_extreme topic.

She has already created the boto3 SNS client in the sns variable. She created a variable departments that contains a unique list of departments.

In this lesson, you will help Sam make the City run faster!

You will create multi-level topics with a different set of subscribers that trigger based on different thresholds.

You will effectively be building a smart alerting system!

"""

dept_arns = {} 

for dept in departments:
  # For each deparment, create a critical topic
  critical = sns.create_topic(Name="{}_critical".format(dept))
  # For each department, create an extreme topic
  extreme = sns.create_topic(Name="{}_extreme".format(dept))
  # Place the created TopicARNs into a dictionary 
  dept_arns['{}_critical'.format(dept)] = critical['TopicArn']
  dept_arns['{}_extreme'.format(dept)] = extreme['TopicArn']

# Print the filled dictionary.
print(dept_arns)

"""
Different protocols per topic level
Now that Sam has created the critical and extreme topics, she needs to subscribe the staff from her contact list into these topics.

Sam decided that the people subscribed to 'critical' topics will only receive emails. On the other hand, people subscribed to 'extreme' topics will receive SMS - because those are pretty urgent.

She has already created the boto3 SNS client in the sns variable.

Help Sam subscribe the users in the contacts DataFrame to email or SMS notifications based on their department. This will help get the right alerts to the right people, making the City of San Diego run better and faster!
"""

for index, user_row in contacts.iterrows():
  # Get topic names for the users's dept
  critical_tname = '{}_critical'.format(user_row['Department'])
  extreme_tname = '{}_extreme'.format(user_row['Department'])
  
  # Get or create the TopicArns for a user's department.
  critical_arn = sns.create_topic(Name=critical_tname)['TopicArn']
  extreme_arn = sns.create_topic(Name=extreme_tname)['TopicArn']
  
  # Subscribe each users email to the critical Topic
  sns.subscribe(TopicArn = critical_arn, 
                Protocol='email', Endpoint=user_row['Email'])
  # Subscribe each users phone number for the extreme Topic
  sns.subscribe(TopicArn = extreme_arn, 
                Protocol='sms', Endpoint=str(user_row['Phone']))




"""
Sending multi-level alerts
Sam is going to prototype her alerting system with the water data and the water department.

According to the Director, when there are over 100 alerts outstanding, that's considered critical. If there are over 300, that's extreme.

She has done some calculations and came up with a vcounts dictionary, that contains current requests for 'water', 'streets' and 'trash'.

She has also already created the boto3 SNS client and stored it in the sns variable.

In this exercise, you will help Sam publish a critical and an extreme alert based on the thresholds!

"""

if vcounts['water'] > 100:
  # If over 100 water violations, publish to water_critical
  sns.publish(
    TopicArn = dept_arns['water_critical'],
    Message = "{} water issues".format(vcounts['water']),
    Subject = "Help fix water violations NOW!")

if vcounts['water'] > 300:
  # If over 300 violations, publish to water_extreme
  sns.publish(
    TopicArn = dept_arns['water_extreme'],
    Message = "{} violations! RUN!".format(vcounts['water']),
    Subject = "THIS IS BAD.  WE ARE FLOODING!")

"""
Rekognizing patterns

So what is Rekognition anyway?
Detecting Objects in an image

Extracting Text from Images
Why not build our own?

Use Rekognition if:
* Quick but good
* Keep code simple
* Recognize many things

Build a model if:

* Custom requirements
* Security implictions
* Large volumes


Inialize S3 Client

s3 = boto3.client(
  's3', region_name='us-east-1',
  aws_access_key_id = AWS_KEY_ID, aws_secret_access_key=AWS_SECRET
)

Upload a file

s3.upload_file(
  Filename = 'report.jpg',
  Key = 'report.jpg',
  Bucket='datacamp-img'
)
Object detection
Iniate the client

rekog = boto3.client(
  'rekognition',
  region_name='us-east-1',
  aws_access_key_id=AWS_KEY_ID,
  aws_secret_access_key=AWS_SECRET
)

response = rekog.detect_labels(
  Image = {'S3Object': {
              'Bucket': 'datacamp-img',
              'Name': 'report.jpg'
  },
  MaxLabels=10,
  MinConfidence=95}
)


Text detection

Perform Detection

response = rekog.detect_text(
  Image={'S3Object':
              {

                'Bucket': 'datacamp-img',
                'Name': 'report.jpg'
  

              }
  }
)
"""

"""
Cat detector
Sam has been getting more and more challenging projects as a result of her popularity and success.

The newest request is from the animal control team. They want to receive notifications when an image comes in from the Get It Done application contains a cat. They can find feral cats and go rescue them. They provided her with two images that she can test her system with. One contains a cat, one does not. Both images are referenced in variables image1 and image2 respectively.

Sam has also created the boto3 Rekognition client in the rekog variable.

Help Sam use Rekognition to enable the animal control team to rescue stray cats!
"""
# Use Rekognition client to detect labels
image1_response = rekog.detect_labels(
    # Specify the image as an S3Object; Return one label
   Image =image1, MaxLabels=1)
"""
{'S3Object': {
              'Bucket': 'datacamp-img',
              'Name': 'report.jpg'
  }
"""

# Print the labels
print(image1_response['Labels'])

"""
The newest request is from the animal control team. They want to receive notifications when an image comes in from the Get It Done application contains a cat. They can find feral cats and go rescue them. They provided her with two images that she can test her system with. One contains a cat, one does not. Both images are referenced in variables image1 and image2 respectively.

Sam has also created the boto3 Rekognition client in the rekog variable.

Help Sam use Rekognition to enable the animal control team to rescue stray cats!

"""
# Print the labels
print(image1_response['Labels'])

# Use Rekognition client to detect labels
image2_response = rekog.detect_labels(Image=image2, MaxLabels=1)

# Print the labels
print(image2_response['Labels'])




"""
Multiple cat detector
After using the Cat Detector for a bit, the Animal Control team found that it was inefficient for them to pursue one cat at a time. It would be better if they could find clusters of cats.

They asked if Sam could add the count of cats detected to the message in the alerts they receive. They also asked her to lower the confidence floor, allowing the system to have more false positives.

"""
# Create an empty counter variable
cats_count = 0
# Iterate over the labels in the response
for label in response['Labels']:
    # Find the cat label, look over the detected instances
    if label['Name'] == 'Cat':
        for instance in label['Instances']:
            # Only count instances with confidence > 85
            if (instance['Confidence'] > 85):
                cats_count += 1
# Print count of cats
print(cats_count)


"""
Parking sign reader
City planners have millions of images from truck cameras. Since the city does not keep a record of this data, parking regulations would be very valuable for planners to know.

Sam found detect_text() in the boto3 Rekognition documentation. She is going to use it to extract text out of the images City planners provided.


Sam has:

Created the Rekognition client.
Called .detect_text() and stored the result in response.
Help Sam create a data source for parking regulations in the City. Use Rekognition's .detect_text() method to extract lines and words from images of parking signs.
"""

# Create empty list of words
words = []
print(response)
# Iterate over the TextDetections in the response dictionary
for text_detection in response['TextDetections']:
    # If TextDetection type is WORD, append it to words list
    if text_detection['Type'] == 'WORD':
        # Append the detected text
        words.append(text_detection['DetectedText'])
# Print out the words list
print(words)


# Create empty list of lines
lines = []


# Iterate over the TextDetections in the response dictionary
for text_detection in response['TextDetections']:
    # If TextDetection type is Line, append it to lines list
    if text_detection['Type'] == 'LINE':
        # Append the detected text
        lines.append(text_detection['DetectedText'])
# Print out the words list
print(lines)




"""
Comprehending text

AWS translate console
Translating text
Initialize Client

translate = boto3.client('translate',
  region_name='us-east-1',
  aws_access_key_id=AWS_KEY_ID, aws_secret_access_key=AWS_SECRET)

  Translate text

  response =  translate.translate_text(
  Text= 'hello, how are you?',

  SourceLanguageCode='auto',
  TargetLanguageCode='es'
  )
Translating text

translated_text = translate.translate_text(
  Text='Hello, how are you?',
  SourceLanguageCode = 'auto',
  TargetLanguageCode = 'es'
)['TranslatedText']


Detecting language

comprehen = boto3.client('comprehend', region_name='us-east-1',
      aws_access_key_id = AWS_KEY_ID, aws_secret_access_key=AWS_SECRET
)

Detect dominant language

response = comprehend.detect_dominant_language(
Text = 'hay basura por todas partes a lo largo de la carretera.'
)

Understanding  sentiment
response = comprehend.detect_sentiment(
  Text='DataCamp students are amazing.',
  LanguageCode='en'
)

sentiment = comprehend.detect_sentiment(
  Text='Maksim is amazing.',
  LanguageCode='en'
)['Sentiment']
"""
"""
Exercise
Exercise
Detecting language
The City Council is wondering whether it's worth it to build a Spanish version of the Get It Done application. There is a large Spanish speaking constituency, but they are not sure if they will engage. Building in multi-lingual translation complicates the system and needs to be justified.

They ask Sam to figure out how many people are posting requests in Spanish.

She has already loaded the CSV into the dumping_df variable and subset it to the following columns:
Help Sam quantify the demand for a Spanish version of the Get It Done application. Figure out how many requesters use Spanish and print the final result!
"""

# For each dataframe row
for index, row in dumping_df.iterrows():
    # Get the public description field
    description =dumping_df.loc[index, 'public_description']
    if description != '':
        # Detect language in the field content
        resp = comprehend.detect_dominant_language(Text=description)
        # print(resp)
        # Assign the top choice language to the lang column.
        dumping_df.loc[index, 'lang'] = resp['Languages'][0]['LanguageCode']
        
# Count the total number of spanish posts
spanish_post_ct = len(dumping_df[dumping_df.lang == 'es'])
# Print the result
print("{} posts in Spanish".format(spanish_post_ct))


"""
Translating Get It Done requests
Often, Get It Done requests come in with multiple languages in the description. This is a challenge for many City teams. In order to review the requests, many city teams need to have a translator on staff, or hope they know someone who speaks the language.

The Streets director asked Sam to help. He wanted her to translate the Get It Done requests by running a job at the end of every day.

Sam decides to run the requests through the AWS translate service. She has already loaded the CSV into the dumping_df variable and subset it to the following columns:

"""

"""
Help Sam translate the requests to Spanish by running them through the AWS translate service!
"""

for index, row in dumping_df.iterrows():

  # Get the public_description into a variable
  description = dumping_df.loc[index, 'public_description']
  if description != '':
      # Translate the public description
      resp = translate.translate_text(
          Text=description, 
          SourceLanguageCode='auto', TargetLanguageCode='en')
      # print(resp)
      # Store original language in original_lang column
      dumping_df.loc[index, 'original_lang'] = resp['SourceLanguageCode']
      # Store the translation in the translated_desc column
      dumping_df.loc[index, 'translated_desc'] = resp['TranslatedText']
# Preview the resulting DataFrame
dumping_df = dumping_df[['service_request_id', 'original_lang', 'translated_desc']]
dumping_df.head()

# TranslatedText


"""
Getting request sentiment
After successfully translating Get It Done cases for the Streets Director, he asked for one more thing. He really wants to understand how people in the City feel about his department's work. She believes she can answer that question via sentiment analysis of Get It Done requests. She has already loaded the CSV into the dumping_df variable and subset it to the following columns:

In this exercise, you will help Sam better understand the moods of the voices of the people that submit Get It Done cases, and whether they are coming into the interaction with the City in a positive mood or a negative one.

"""
for index, row in dumping_df.iterrows():
    # Get the translated_desc into a variable
    description = dumping_df.loc[index, 'public_description']
    if description != '':
        # Get the detect_sentiment response
        response = comprehend.detect_sentiment(
          Text=description, 
          LanguageCode='en')
        print(response)
        # Get the sentiment key value into sentiment column
        dumping_df.loc[index, 'sentiment'] = response['Sentiment']
# Preview the dataframe
dumping_df.head()




"""
Case Study:
Scooting Around

Inialize boto3 service clients.
Initialize rekognition client

rekog = boto3.client('rekognition',
                      region_name='us-east-1',
                      aws_access_key_id=AWS_KEY_ID,
                      aws_secret_access_key=AWS_SECRECT
)


Initialize comprehend client

comprehend = boto3.client('comprehend',
            region_name='us-east-1',
            aws-access_key_id=AWS_KEY_ID,
            aws_secrect_access_key=AWS_SECRET 
)

Initialize translate client

translate = boto3.client('translate',
                        region_name = 'us-east-1',
                        aws_acces_key_id=AWS_KEY_ID,
                        aws_secret_access_key=AWS_SECRET
)
Translate all descriptions to English

for index, row in df.iterrows():
  desc = df.loc[index, 'public_description']
  if desc != '':
    resp = translate_fake.translate_text(
      Text=desc,
      SourceLanguageCode='auto',
      TargetLanguageCode='en'
    )
  df.loc[index, 'public_description'] = resp['TranslatedText']


Detect Text Sentiment

for index, row in df.iterrows():

  desc = df.loc[index, 'public_description']
  if desc != '':
    resp = comprehend.detect_sentiment(
      Text=desc,
      LanguageCode='en'
    )
  df.loc[index, 'sentiment'] = resp['Sentiment']

Detect Scooter in Image

df['img_scooter'] = 0

for index, row in df.iterrows():
  image = df.loc[index, 'image']
  response = rekog.detect_labels(
  # Specify the image as an S3Object
  image = {'S3Object':{'Bucket':'gid-images', 'Name':image}}
  )
  for label in response['Labels']:
    if label['Name'] == 'Scooter':
      df.loc[index,'img_scooter']= 1
      break

Final count!

Select only rows where there was a scooter image and that have negative sentiment

pickups = df[((df.img_scooter==1) & (df.sentiment=='NEGATIVE'))]

num_pickups = len(pickups)
"""
"""
Scooter community sentiment
The City Council is curious about how different communities in the City are reacting to the Scooters. The dataset has expanded since Sam's initial analysis, and now contains Vietnamese, Tagalog, Spanish and English reports.

They ask Sam to see if she can figure it out. She decides that the best way to proxy for a community is through language (at least with the data she immediately has access to).

She has already loaded the CSV into the scooter_df variable:
In this exercise, you will help Sam understand sentiment across many different languages. This will help the City understand how different communities are relating to scooters, something that will affect the votes of City Council members.
"""

for index, row in scooter_requests.iterrows():
    # For every DataFrame row
    desc = scooter_requests.loc[index, 'public_description']
    if desc != '':
        # Detect the dominant language
        resp = comprehend.detect_dominant_language(Text=desc)
        lang_code = resp['Languages'][0]['LanguageCode']
        scooter_requests.loc[index, 'lang'] = lang_code
        # Use the detected language to determine sentiment
        scooter_requests.loc[index, 'sentiment'] = comprehend.detect_sentiment(
          Text=desc, 
          LanguageCode=lang_code)['Sentiment']
# Perform a count of sentiment by group.
counts = scooter_requests.groupby(['sentiment', 'lang']).count()
counts.head()


"""
Scooter dispatch
The City Council were huge fans of Sam's prediction about whether scooter was blocking a sidewalk or not. So much so, they asked her to build a notification system to dispatch crews to impound scooters from sidewalks.

With the dataset she created, Sam can dispatch crews to the case's coordinates when a request has negative sentiment.


In this exercise, you will help Sam implement a system that dispatches crews based on sentiment and image recognition. You will help Sam pair human and machine for effective City management!
"""

# Get topic ARN for scooter notifications
topic_arn = sns.create_topic(Name='scooter_notifications')['TopicArn']

for index, row in scooter_requests.iterrows():
    # Check if notification should be sent
    if (row['sentiment'] == 'NEGATIVE') & (row['img_scooter'] == 1):
        # Construct a message to publish to the scooter team.
        message = "Please remove scooter at {}, {}. Description: {}".format(
            row['long'], row['lat'], row['public_description'])

        # Publish the message to the topic!
        sns.publish( TopicArn = topic_arn,
                    Message= message, 
                    Subject = "Scooter Alert")